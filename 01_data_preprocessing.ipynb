{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e0f61f",
   "metadata": {},
   "source": [
    "**Enhancing Customer Retention through Machine Learning**: <br> Segmentation, Churn Prediction, and Product Recommendation – A Case Study on Flipkart Data.\n",
    "<br><br>\n",
    "Phase: **Data Preprocessing** <br>\n",
    "By: Gia-My Nguyen <br>\n",
    "Last updated (dd/mm/yyyy): 25/09/2025\n",
    "<br><br>\n",
    "\n",
    "**Completed:**\n",
    "- Dropped redundant ('unnamed') columns & NULLs in *total_weighted_landing_price*.\n",
    "- Identified \"0\" values in *procured_quantity*, *unit_selling_price*.\n",
    "- Remove records with procured_quantity = 0, keep only those > 0.\"\n",
    "- Checked consistency between category and category_id, handled the 'Syrups' case.\n",
    "- Supporting files for easier cross-referencing (Location: data/supporting).\n",
    "<br><br>\n",
    "\n",
    "**Tasks Remaining**\n",
    "- Handle \"0\" values in *unit_selling_price*\n",
    "- Standardize str columns (before modelling)\n",
    "- Create 'Revenue' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c56ef383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10efcfbd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9efa15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Convert all specified ID columns to string type.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        id_columns (list, optional): List of ID columns to convert. \n",
    "                                     If None, will auto-detect columns ending with '_id'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with converted ID columns\n",
    "\"\"\"\n",
    "    \n",
    "def convert_id_to_str(df, id_columns=None):\n",
    "    if id_columns is None:\n",
    "        id_columns = [col for col in df.columns if col.endswith(\"_id\") or col == 'dim_customer_key']\n",
    "    \n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "267af6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Create a mapping between 'category_id' and category_name'.  \n",
    "    If a category name has multiple IDs -> append the ID to the name.\n",
    "\"\"\"\n",
    "\n",
    "def build_mapping(df, id_col, name_col):\n",
    "    \n",
    "    # Category name with multiple IDs\n",
    "    dup_names = (\n",
    "        df.groupby(name_col)[id_col]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "    dup_names = dup_names[dup_names[id_col] > 1][name_col].tolist()\n",
    "\n",
    "    # Handle duplicated names\n",
    "    def rename(row):\n",
    "        if row[name_col] in dup_names:\n",
    "            return f\"{row[name_col]} ({row[id_col]})\"\n",
    "        return row[name_col]\n",
    "\n",
    "    df[name_col] = df.apply(rename, axis=1)\n",
    "\n",
    "    # mapping data\n",
    "    mapping = (\n",
    "        df[[id_col, name_col]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c80eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_double_quotes(df, col_name):\n",
    "#     \"\"\"\n",
    "#     Clean column:\n",
    "#     - Remove surrounding double quotes\n",
    "#     - Remove all extra quotes inside text\n",
    "#     - Strip leading/trailing spaces\n",
    "#     - Normalize multiple spaces into one\n",
    "#     \"\"\"\n",
    "#     df[col_name] = (\n",
    "#         df[col_name]\n",
    "#         .astype(str)                                  # đảm bảo dữ liệu là string\n",
    "#         .str.strip('\"')                               # bỏ \" ở đầu và cuối\n",
    "#         # .str.replace('\"', '', regex=False)            # bỏ \" còn sót bên trong\n",
    "#         .str.strip()                                  # bỏ khoảng trắng đầu/cuối\n",
    "#         .str.replace(r'\\s+', ' ', regex=True)         # chuẩn hóa khoảng trắng\n",
    "#     )\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2aaff",
   "metadata": {},
   "source": [
    "# Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45e3629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "\n",
    "df_sales = pd.read_csv('Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16bf2c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46706387 entries, 0 to 46706386\n",
      "Data columns (total 13 columns):\n",
      " #   Column                        Dtype  \n",
      "---  ------                        -----  \n",
      " 0   Unnamed: 0.2                  int64  \n",
      " 1   Unnamed: 0.1                  int64  \n",
      " 2   Unnamed: 0                    int64  \n",
      " 3   date_                         object \n",
      " 4   city_name                     object \n",
      " 5   order_id                      int64  \n",
      " 6   cart_id                       int64  \n",
      " 7   dim_customer_key              int64  \n",
      " 8   procured_quantity             int64  \n",
      " 9   unit_selling_price            float64\n",
      " 10  total_discount_amount         float64\n",
      " 11  product_id                    int64  \n",
      " 12  total_weighted_landing_price  float64\n",
      "dtypes: float64(3), int64(8), object(2)\n",
      "memory usage: 4.5+ GB\n"
     ]
    }
   ],
   "source": [
    "# Basic information\n",
    "\n",
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5bafdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"Unnamed\" columns\n",
    "\n",
    "cols_to_drop = [col for col in df_sales.columns if 'Unnamed' in col]\n",
    "df_sales = df_sales.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5dbc9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_                               0\n",
       "city_name                           0\n",
       "order_id                            0\n",
       "cart_id                             0\n",
       "dim_customer_key                    0\n",
       "procured_quantity                   0\n",
       "unit_selling_price                  0\n",
       "total_discount_amount               0\n",
       "product_id                          0\n",
       "total_weighted_landing_price    79355\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "\n",
    "df_sales.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423f60c",
   "metadata": {},
   "source": [
    "## Problem 01: <br> **79,355 missing values in \"Total Weighted Landing Price\"**\n",
    "\n",
    "File: sales_missing_values.csv\n",
    "\n",
    "**Proposal**\n",
    "\n",
    "- Remove all these records from the master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73724fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_missing_values.csv\n",
    "\n",
    "# sales_missing_values = df_sales[df_sales['total_weighted_landing_price'].isnull()]\n",
    "# print(sales_missing_values.shape[0])\n",
    "\n",
    "# sales_missing_values.to_csv('data/anomalies/sales_missing_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12603474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after dropping NULL in total_weighted_landing_price: 46627032\n"
     ]
    }
   ],
   "source": [
    "# Drop all records where total_weighted_landing_price is NULL\n",
    "\n",
    "df_sales_cleaned = df_sales.dropna(subset=['total_weighted_landing_price'])\n",
    "print(\"Count after dropping NULL in total_weighted_landing_price:\", len(df_sales_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3fdac206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_                           0\n",
       "city_name                       0\n",
       "order_id                        0\n",
       "cart_id                         0\n",
       "dim_customer_key                0\n",
       "procured_quantity               0\n",
       "unit_selling_price              0\n",
       "total_discount_amount           0\n",
       "product_id                      0\n",
       "total_weighted_landing_price    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NULL values again\n",
    "\n",
    "df_sales_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beb89280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: procured_quantity\n",
      "  Records = 0: 178908\n",
      "  Records < 0: 0\n",
      "\n",
      "Column: unit_selling_price\n",
      "  Records = 0: 48269\n",
      "  Records < 0: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check abnormal data in 'procured_quantity' & 'unit_selling_price'\n",
    "\n",
    "cols_to_check = ['procured_quantity', 'unit_selling_price']\n",
    "\n",
    "for col in cols_to_check:\n",
    "    zero_count = (df_sales_cleaned[col] == 0).sum()\n",
    "    negative_count = (df_sales_cleaned[col] < 0).sum()\n",
    "    \n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Records = 0: {zero_count}\")\n",
    "    print(f\"  Records < 0: {negative_count}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6159a63",
   "metadata": {},
   "source": [
    "## Problem 02: <br> **178,908 records with procured_quantity = 0**\n",
    "File: sales_zero_quantity.csv <br> <br>\n",
    "No products were purchased -> Invalid transactions <br>\n",
    "Possible reason: system error\n",
    "\n",
    "**Proposal**: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "348f6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_zero_quantity.csv\n",
    "\n",
    "# sales_zero_quantity = df_sales_cleaned[df_sales_cleaned['procured_quantity'] == 0]\n",
    "# sales_zero_quantity.to_csv('data/anomalies/sales_zero_quantity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df04d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records w. 'procured_quantity=0':  0\n"
     ]
    }
   ],
   "source": [
    "# Only get records with procured_quantity > 0\n",
    "\n",
    "df_sales_cleaned = df_sales_cleaned[df_sales_cleaned['procured_quantity'] > 0]\n",
    "\n",
    "# check agian\n",
    "print(\"Records w. 'procured_quantity=0': \",df_sales_cleaned[df_sales_cleaned['procured_quantity'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cccd8",
   "metadata": {},
   "source": [
    "## Problem 03: <br> **48,269 records with unit_selling_price = 0**\n",
    "File: sales_zero_unit_price.csv<br> <br>\n",
    "Products sold at zero price <br>\n",
    "Possible reasons:\n",
    "- Sample products / free gifts / special promotions\n",
    "- Error in recording selling price\n",
    "\n",
    "**Proposal**: add flag *is_free_product*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d7205c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_zero_unit_price.csv\n",
    "\n",
    "# sales_zero_unit_price = df_sales_cleaned[df_sales_cleaned['unit_selling_price'] == 0]\n",
    "# sales_zero_unit_price.to_csv('data/anomalies/sales_zero_unit_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0074270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the date column and convert to datetime objects\n",
    "\n",
    "df_sales_cleaned.rename(columns={'date_': 'date'}, inplace=True)\n",
    "df_sales_cleaned['date'] = pd.to_datetime(df_sales_cleaned['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6e1ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ids to str\n",
    "\n",
    "df_sales_cleaned = convert_id_to_str(df_sales_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34f00467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 46448124 entries, 0 to 46706386\n",
      "Data columns (total 10 columns):\n",
      " #   Column                        Dtype         \n",
      "---  ------                        -----         \n",
      " 0   date                          datetime64[ns]\n",
      " 1   city_name                     object        \n",
      " 2   order_id                      object        \n",
      " 3   cart_id                       object        \n",
      " 4   dim_customer_key              object        \n",
      " 5   procured_quantity             int64         \n",
      " 6   unit_selling_price            float64       \n",
      " 7   total_discount_amount         float64       \n",
      " 8   product_id                    object        \n",
      " 9   total_weighted_landing_price  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), int64(1), object(5)\n",
      "memory usage: 3.8+ GB\n"
     ]
    }
   ],
   "source": [
    "df_sales_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4c916ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city_name</th>\n",
       "      <th>order_id</th>\n",
       "      <th>cart_id</th>\n",
       "      <th>dim_customer_key</th>\n",
       "      <th>procured_quantity</th>\n",
       "      <th>unit_selling_price</th>\n",
       "      <th>total_discount_amount</th>\n",
       "      <th>product_id</th>\n",
       "      <th>total_weighted_landing_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>112246974</td>\n",
       "      <td>173273802</td>\n",
       "      <td>17995199</td>\n",
       "      <td>1</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>344107</td>\n",
       "      <td>202.513030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>112246976</td>\n",
       "      <td>173273597</td>\n",
       "      <td>18259433</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>389676</td>\n",
       "      <td>48.714375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>112247019</td>\n",
       "      <td>173123717</td>\n",
       "      <td>5402601</td>\n",
       "      <td>1</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39411</td>\n",
       "      <td>975.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>HR-NCR</td>\n",
       "      <td>112247045</td>\n",
       "      <td>172547459</td>\n",
       "      <td>15649744</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369742</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>112247123</td>\n",
       "      <td>173081820</td>\n",
       "      <td>10127605</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12872</td>\n",
       "      <td>57.980004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  city_name   order_id    cart_id dim_customer_key  \\\n",
       "0 2022-04-01     Mumbai  112246974  173273802         17995199   \n",
       "1 2022-04-01  Bengaluru  112246976  173273597         18259433   \n",
       "2 2022-04-01  Bengaluru  112247019  173123717          5402601   \n",
       "3 2022-04-01     HR-NCR  112247045  172547459         15649744   \n",
       "4 2022-04-01     Mumbai  112247123  173081820         10127605   \n",
       "\n",
       "   procured_quantity  unit_selling_price  total_discount_amount product_id  \\\n",
       "0                  1               234.0                    0.0     344107   \n",
       "1                  1                64.0                    0.0     389676   \n",
       "2                  1              1031.0                    0.0      39411   \n",
       "3                  1                57.0                    0.0     369742   \n",
       "4                  2                30.0                    0.0      12872   \n",
       "\n",
       "   total_weighted_landing_price  \n",
       "0                    202.513030  \n",
       "1                     48.714375  \n",
       "2                    975.996000  \n",
       "3                     25.000000  \n",
       "4                     57.980004  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb24e17",
   "metadata": {},
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a4b2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get products data\n",
    "\n",
    "df_products = pd.read_csv('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b604c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32226 entries, 0 to 32225\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Unnamed: 0         32226 non-null  int64 \n",
      " 1   product_id         32226 non-null  int64 \n",
      " 2   product_name       32226 non-null  object\n",
      " 3   unit               32226 non-null  object\n",
      " 4   product_type       32226 non-null  object\n",
      " 5   brand_name         30788 non-null  object\n",
      " 6   manufacturer_name  29810 non-null  object\n",
      " 7   l0_category        32226 non-null  object\n",
      " 8   l1_category        32226 non-null  object\n",
      " 9   l2_category        32226 non-null  object\n",
      " 10  l0_category_id     32226 non-null  int64 \n",
      " 11  l1_category_id     32226 non-null  int64 \n",
      " 12  l2_category_id     32226 non-null  int64 \n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Basic information\n",
    "\n",
    "df_products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7eba95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"Unnamed\" columns\n",
    "\n",
    "cols_to_drop = [col for col in df_products.columns if 'Unnamed' in col]\n",
    "df_products = df_products.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2429816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id              0\n",
       "product_name            0\n",
       "unit                    0\n",
       "product_type            0\n",
       "brand_name           1438\n",
       "manufacturer_name    2416\n",
       "l0_category             0\n",
       "l1_category             0\n",
       "l2_category             0\n",
       "l0_category_id          0\n",
       "l1_category_id          0\n",
       "l2_category_id          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values\n",
    "\n",
    "df_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494cc5f",
   "metadata": {},
   "source": [
    "## Problem 04: <br> **Missing values in 'brand_name' & 'manufacturer_name'**\n",
    "1. 'brand_name'         : 1,438 missing values\n",
    "2. 'manufacturer_name'  : 2,416 missing values   \n",
    "\n",
    "**Proposal:**\n",
    "- Customer Segmentation and Churn Prediction -> No Problem!  \n",
    "- Product Recommendation: assign 'Unknown'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "366c1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with 'Unknown'\n",
    "\n",
    "df_products['brand_name'] = df_products['brand_name'].fillna(\"Unknown\")\n",
    "df_products['manufacturer_name'] = df_products['manufacturer_name'].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ad628d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated product_id: 0\n",
      "Total duplicated values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the number of duplicate product_id values\n",
    "\n",
    "duplicate_products = df_products[df_products.duplicated(subset=['product_id'], keep=False)]\n",
    "\n",
    "print(\"Duplicated product_id:\", duplicate_products['product_id'].nunique())\n",
    "print(\"Total duplicated values:\", len(duplicate_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eb7c1305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 0\n"
     ]
    }
   ],
   "source": [
    "# Check product_name contains numeric-only values\n",
    "\n",
    "mask = df_products[\"product_name\"].astype(str).str.isnumeric()\n",
    "numeric_names = df_products[mask]\n",
    "\n",
    "print(f\"Count: {numeric_names.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "98d9fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ids to str\n",
    "\n",
    "df_products = convert_id_to_str(df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "edc3ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32226 entries, 0 to 32225\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   product_id         32226 non-null  object\n",
      " 1   product_name       32226 non-null  object\n",
      " 2   unit               32226 non-null  object\n",
      " 3   product_type       32226 non-null  object\n",
      " 4   brand_name         32226 non-null  object\n",
      " 5   manufacturer_name  32226 non-null  object\n",
      " 6   l0_category        32226 non-null  object\n",
      " 7   l1_category        32226 non-null  object\n",
      " 8   l2_category        32226 non-null  object\n",
      " 9   l0_category_id     32226 non-null  object\n",
      " 10  l1_category_id     32226 non-null  object\n",
      " 11  l2_category_id     32226 non-null  object\n",
      "dtypes: object(12)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_products.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383cabb",
   "metadata": {},
   "source": [
    "***Consistency check between category IDs and category names*** <br>\n",
    "3 levels: l0, l1, l2\n",
    "\n",
    "**What it does:**\n",
    "1. Check if each category ID maps to only one category name.\n",
    "2. Check if each category name maps to only one category ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b80c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- L0 ---\n",
      "[OK]: ID -> only 1 category_name\n",
      "[OK]: category_name → only 1 ID\n",
      "\n",
      "--- L1 ---\n",
      "[OK]: ID -> only 1 category_name\n",
      "[OK]: category_name → only 1 ID\n",
      "\n",
      "--- L2 ---\n",
      "[OK]: ID -> only 1 category_name\n",
      "[Inconsistent]: category_name → multiple IDs:\n",
      "l2_category\n",
      "Syrups    2\n",
      "Name: l2_category_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Consistency check between category IDs and category names\n",
    "\n",
    "# Cross-check consistency between *_category_id and *_category\n",
    "for level in [\"l0\", \"l1\", \"l2\"]:\n",
    "    check = df_products.groupby(f\"{level}_category_id\")[f\"{level}_category\"].nunique()\n",
    "    inconsistent_ids = check[check > 1]\n",
    "    \n",
    "    print(f\"--- {level.upper()} ---\")\n",
    "    if inconsistent_ids.empty:\n",
    "        print(\"[OK]: ID -> only 1 category_name\")\n",
    "    else:\n",
    "        print(\"[Inconsistent]: ID → multiple category_names:\")\n",
    "        print(inconsistent_ids)\n",
    "\n",
    "    check_name = df_products.groupby(f\"{level}_category\")[f\"{level}_category_id\"].nunique()\n",
    "    inconsistent_names = check_name[check_name > 1]\n",
    "\n",
    "    if inconsistent_names.empty:\n",
    "        print(\"[OK]: category_name → only 1 ID\\n\")\n",
    "    else:\n",
    "        print(\"[Inconsistent]: category_name → multiple IDs:\")\n",
    "        print(inconsistent_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899b745",
   "metadata": {},
   "source": [
    "**The above output shows:**\n",
    "- L0 and L1 are fully consistent.\n",
    "- L2 has a problem: \"Syrups\" is mapped to 2 different IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b936dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  l2_category l2_category_id  count_products\n",
      "0      Syrups           1136              43\n",
      "1      Syrups           1289              31\n"
     ]
    }
   ],
   "source": [
    "# Filtered Syrups-only\n",
    "\n",
    "syrups_detail = (\n",
    "    df_products[df_products[\"l2_category\"] == \"Syrups\"]\n",
    "    .groupby([\"l2_category\", \"l2_category_id\"])[\"product_id\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"product_id\": \"count_products\"})\n",
    ")\n",
    "\n",
    "print(syrups_detail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd98ad",
   "metadata": {},
   "source": [
    "## Problem 05: <br> **Duplicate category name with multiple IDs in L2**\n",
    "Issue: The l2_category \"Syrups\" is linked to two different l2_category_id values <br>\n",
    "-> category ID - name inconsistency. \n",
    "\n",
    "**Proposal:**\n",
    "- l2_category_id = 1136 -> \"Syrups (1136)\"\n",
    "- l2_category_id = 1289 -> \"Syrups (1289)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be8bbcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "\n",
    "df_products_cleaned = df_products.copy()\n",
    "\n",
    "l0_mapping = build_mapping(df_products_cleaned, \"l0_category_id\", \"l0_category\")\n",
    "l1_mapping = build_mapping(df_products_cleaned, \"l1_category_id\", \"l1_category\")\n",
    "l2_mapping = build_mapping(df_products_cleaned, \"l2_category_id\", \"l2_category\")\n",
    "\n",
    "# l0_mapping.to_csv(\"data/supporting/l0_category.csv\", index=False)\n",
    "# l1_mapping.to_csv(\"data/supporting/l1_category.csv\", index=False)\n",
    "# l2_mapping.to_csv(\"data/supporting/l2_category.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "923dd2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>unit</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>manufacturer_name</th>\n",
       "      <th>l0_category</th>\n",
       "      <th>l1_category</th>\n",
       "      <th>l2_category</th>\n",
       "      <th>l0_category_id</th>\n",
       "      <th>l1_category_id</th>\n",
       "      <th>l2_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30623</th>\n",
       "      <td>487147</td>\n",
       "      <td>Hershey's Genuine Chocolate Syrup - Pack of 2</td>\n",
       "      <td>2 x 1.3 kg</td>\n",
       "      <td>Syrup</td>\n",
       "      <td>Hershey's</td>\n",
       "      <td>HERSHEY INDIA PRIVATE LIMITED</td>\n",
       "      <td>Sweet Tooth</td>\n",
       "      <td>Syrups</td>\n",
       "      <td>Syrups (1289)</td>\n",
       "      <td>9</td>\n",
       "      <td>1289</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30624</th>\n",
       "      <td>490896</td>\n",
       "      <td>Del Monte Chocolate Flavoured Syrup</td>\n",
       "      <td>600 g</td>\n",
       "      <td>Syrup</td>\n",
       "      <td>Del Monte</td>\n",
       "      <td>Del Monte Foods Pvt. Ltd</td>\n",
       "      <td>Sweet Tooth</td>\n",
       "      <td>Syrups</td>\n",
       "      <td>Syrups (1289)</td>\n",
       "      <td>9</td>\n",
       "      <td>1289</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30640</th>\n",
       "      <td>479638</td>\n",
       "      <td>Hershey's Shell Chocolate Syrup</td>\n",
       "      <td>205 g</td>\n",
       "      <td>Syrup</td>\n",
       "      <td>Hershey's</td>\n",
       "      <td>HERSHEY INDIA PRIVATE LIMITED</td>\n",
       "      <td>Sweet Tooth</td>\n",
       "      <td>Syrups</td>\n",
       "      <td>Syrups (1289)</td>\n",
       "      <td>9</td>\n",
       "      <td>1289</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id                                   product_name        unit  \\\n",
       "30623     487147  Hershey's Genuine Chocolate Syrup - Pack of 2  2 x 1.3 kg   \n",
       "30624     490896            Del Monte Chocolate Flavoured Syrup       600 g   \n",
       "30640     479638                Hershey's Shell Chocolate Syrup       205 g   \n",
       "\n",
       "      product_type brand_name              manufacturer_name  l0_category  \\\n",
       "30623        Syrup  Hershey's  HERSHEY INDIA PRIVATE LIMITED  Sweet Tooth   \n",
       "30624        Syrup  Del Monte       Del Monte Foods Pvt. Ltd  Sweet Tooth   \n",
       "30640        Syrup  Hershey's  HERSHEY INDIA PRIVATE LIMITED  Sweet Tooth   \n",
       "\n",
       "      l1_category    l2_category l0_category_id l1_category_id l2_category_id  \n",
       "30623      Syrups  Syrups (1289)              9           1289           1289  \n",
       "30624      Syrups  Syrups (1289)              9           1289           1289  \n",
       "30640      Syrups  Syrups (1289)              9           1289           1289  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "# df_products_cleaned[df_products_cleaned['l2_category'] == 'Syrups (1136)'].head(3)\n",
    "df_products_cleaned[df_products_cleaned['l2_category'] == 'Syrups (1289)'].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07744f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "\n",
    "# Unique product names\n",
    "unique_products = df_products_cleaned[[\"product_id\", \"product_name\"]].drop_duplicates()\n",
    "\n",
    "unique_products.to_csv(\"data/supporting/unique_products.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32f431e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32226 entries, 0 to 32225\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   product_id         32226 non-null  object\n",
      " 1   product_name       32226 non-null  object\n",
      " 2   unit               32226 non-null  object\n",
      " 3   product_type       32226 non-null  object\n",
      " 4   brand_name         32226 non-null  object\n",
      " 5   manufacturer_name  32226 non-null  object\n",
      " 6   l0_category        32226 non-null  object\n",
      " 7   l1_category        32226 non-null  object\n",
      " 8   l2_category        32226 non-null  object\n",
      " 9   l0_category_id     32226 non-null  object\n",
      " 10  l1_category_id     32226 non-null  object\n",
      " 11  l2_category_id     32226 non-null  object\n",
      "dtypes: object(12)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_products_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d463e",
   "metadata": {},
   "source": [
    "## Problem 06: <br> **1,478 *product_id* present in *Sales* but missing in *Products***\n",
    "\n",
    "\n",
    "\n",
    "Number of *product_id* in *Sales* not found in *Products*: **1,478** <br>\n",
    "-> The products metadata have not recorded <br>\n",
    "**Proposal:** add dummy products to *Products*\n",
    "\n",
    "Number of *product_id* in *Products* not found in *Sales*: **16,461** <br>\n",
    "-> It can be unsold products. Totally fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b426886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of product_id in *Sales*: 17243\n",
      "Count of product_id in *Products*: 32226\n",
      "---\n",
      "[WARNING]\n",
      " Count of product_id in *Sales* not found in *Products*: 1478\n",
      "[INFO]\n",
      " Count of product_id in *Products* not found in *Sales*: 16461\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "print(\"Count of product_id in *Sales*:\", df_sales_cleaned[\"product_id\"].unique().shape[0])\n",
    "print(\"Count of product_id in *Products*:\", df_products_cleaned[\"product_id\"].unique().shape[0])\n",
    "print(\"---\")\n",
    "\n",
    "sales_products = set(df_sales_cleaned[\"product_id\"].unique())\n",
    "products_master = set(df_products_cleaned[\"product_id\"].unique())\n",
    "\n",
    "missing_in_products = sales_products - products_master\n",
    "print(f\"[WARNING]\\n Count of product_id in *Sales* not found in *Products*: {len(missing_in_products)}\")\n",
    "pd.Series(list(missing_in_products)).to_csv(\"data/supporting/missing_in_products.csv\", index=False)\n",
    "\n",
    "\n",
    "unused_in_sales = products_master - sales_products\n",
    "print(f\"[INFO]\\n Count of product_id in *Products* not found in *Sales*: {len(unused_in_sales)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e186d660",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data\\anomalies'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m orders_with_missing_products \u001b[38;5;241m=\u001b[39m df_sales_cleaned[\n\u001b[0;32m      5\u001b[0m     df_sales_cleaned[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(missing_in_products)\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# to CSV\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43morders_with_missing_products\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/anomalies/orders_missing_in_products.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[OK] Exported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morders_with_missing_products\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m orders to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morders_missing_in_products.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data\\anomalies'"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# flitering\n",
    "orders_with_missing_products = df_sales_cleaned[\n",
    "    df_sales_cleaned[\"product_id\"].isin(missing_in_products)\n",
    "]\n",
    "\n",
    "# to CSV\n",
    "orders_with_missing_products.to_csv(\"data/anomalies/orders_missing_in_products.csv\", index=False)\n",
    "\n",
    "print(f\"[OK] Exported {orders_with_missing_products.shape[0]} orders to 'orders_missing_in_products.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1478\n"
     ]
    }
   ],
   "source": [
    "print(orders_with_missing_products['product_id'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics on the number of orders for each product_id missing in Products.\n",
    "\n",
    "missing_product_stats = (\n",
    "    orders_with_missing_products.groupby(\"product_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"order_count\")\n",
    "    .sort_values(by=\"order_count\", ascending=False)\n",
    ")\n",
    "\n",
    "missing_product_stats.to_csv(\n",
    "    \"data/supporting/missing_product_stats.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy products\n",
    "\n",
    "df_dummy = pd.DataFrame({\n",
    "    \"product_id\": list(missing_in_products),\n",
    "    \"product_name\": [f\"Unknown {pid}\" for pid in missing_in_products],\n",
    "    \"unit\": \"Unknown\",\n",
    "    \"product_type\": \"Unknown\",\n",
    "    \"brand_name\": \"Unknown\",\n",
    "    \"manufacturer_name\": \"Unknown\",\n",
    "    \"l0_category\": \"Unknown\",\n",
    "    \"l1_category\": \"Unknown\",\n",
    "    \"l2_category\": \"Unknown\",\n",
    "    \"l0_category_id\": \"Unknown\",\n",
    "    \"l1_category_id\": \"Unknown\",\n",
    "    \"l2_category_id\": \"Unknown\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc66ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>l0_category</th>\n",
       "      <th>l1_category</th>\n",
       "      <th>l2_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475713</td>\n",
       "      <td>Unknown 475713</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>448554</td>\n",
       "      <td>Unknown 448554</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194125</td>\n",
       "      <td>Unknown 194125</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>484732</td>\n",
       "      <td>Unknown 484732</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>483879</td>\n",
       "      <td>Unknown 483879</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id    product_name l0_category l1_category l2_category\n",
       "0     475713  Unknown 475713     Unknown     Unknown     Unknown\n",
       "1     448554  Unknown 448554     Unknown     Unknown     Unknown\n",
       "2     194125  Unknown 194125     Unknown     Unknown     Unknown\n",
       "3     484732  Unknown 484732     Unknown     Unknown     Unknown\n",
       "4     483879  Unknown 483879     Unknown     Unknown     Unknown"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy products to Products\n",
    "\n",
    "df_products_cleaned_extended = pd.concat([df_products_cleaned, df_dummy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 33704 entries, 0 to 1477\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   product_id         33704 non-null  object\n",
      " 1   product_name       33704 non-null  object\n",
      " 2   unit               32226 non-null  object\n",
      " 3   product_type       32226 non-null  object\n",
      " 4   brand_name         32226 non-null  object\n",
      " 5   manufacturer_name  32226 non-null  object\n",
      " 6   l0_category        33704 non-null  object\n",
      " 7   l1_category        33704 non-null  object\n",
      " 8   l2_category        33704 non-null  object\n",
      " 9   l0_category_id     32226 non-null  object\n",
      " 10  l1_category_id     32226 non-null  object\n",
      " 11  l2_category_id     32226 non-null  object\n",
      "dtypes: object(12)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_products_cleaned_extended.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d76812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>unit</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>manufacturer_name</th>\n",
       "      <th>l0_category</th>\n",
       "      <th>l1_category</th>\n",
       "      <th>l2_category</th>\n",
       "      <th>l0_category_id</th>\n",
       "      <th>l1_category_id</th>\n",
       "      <th>l2_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476763</td>\n",
       "      <td>Christmas - Card</td>\n",
       "      <td>1 unit</td>\n",
       "      <td>Card</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>HOT</td>\n",
       "      <td>Specials</td>\n",
       "      <td>Bill Breaker</td>\n",
       "      <td>Bill Breaker</td>\n",
       "      <td>343</td>\n",
       "      <td>1741</td>\n",
       "      <td>1741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483436</td>\n",
       "      <td>Plum BodyLovin' Hawaiian Rumba Shower Gel - Sa...</td>\n",
       "      <td>20 ml</td>\n",
       "      <td>Sample</td>\n",
       "      <td>Plum BodyLovin'</td>\n",
       "      <td>Pureplay Skin Sciences India Pvt. Ltd.</td>\n",
       "      <td>Specials</td>\n",
       "      <td>Free Store</td>\n",
       "      <td>Free Store</td>\n",
       "      <td>343</td>\n",
       "      <td>1493</td>\n",
       "      <td>1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>476825</td>\n",
       "      <td>Diwali Gift Card Free - Sample</td>\n",
       "      <td>1 unit</td>\n",
       "      <td>Sample</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>HOT</td>\n",
       "      <td>Specials</td>\n",
       "      <td>Bill Breaker</td>\n",
       "      <td>Bill Breaker</td>\n",
       "      <td>343</td>\n",
       "      <td>1741</td>\n",
       "      <td>1741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>483438</td>\n",
       "      <td>Plum BodyLovin' Trippin' Mimosas Shower Gel - ...</td>\n",
       "      <td>20 ml</td>\n",
       "      <td>Sample</td>\n",
       "      <td>Plum BodyLovin'</td>\n",
       "      <td>Pureplay Skin Sciences India Pvt. Ltd.</td>\n",
       "      <td>Specials</td>\n",
       "      <td>Free Store</td>\n",
       "      <td>Free Store</td>\n",
       "      <td>343</td>\n",
       "      <td>1493</td>\n",
       "      <td>1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480473</td>\n",
       "      <td>Flipkart Valentine Day Greeting - Card</td>\n",
       "      <td>1 unit</td>\n",
       "      <td>Card</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Dummy Manufacturer</td>\n",
       "      <td>Specials</td>\n",
       "      <td>Bill Breaker</td>\n",
       "      <td>Bill Breaker</td>\n",
       "      <td>343</td>\n",
       "      <td>1741</td>\n",
       "      <td>1741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id                                       product_name    unit  \\\n",
       "0     476763                                   Christmas - Card  1 unit   \n",
       "1     483436  Plum BodyLovin' Hawaiian Rumba Shower Gel - Sa...   20 ml   \n",
       "2     476825                     Diwali Gift Card Free - Sample  1 unit   \n",
       "3     483438  Plum BodyLovin' Trippin' Mimosas Shower Gel - ...   20 ml   \n",
       "4     480473             Flipkart Valentine Day Greeting - Card  1 unit   \n",
       "\n",
       "  product_type       brand_name                       manufacturer_name  \\\n",
       "0         Card          Unknown                                     HOT   \n",
       "1       Sample  Plum BodyLovin'  Pureplay Skin Sciences India Pvt. Ltd.   \n",
       "2       Sample          Unknown                                     HOT   \n",
       "3       Sample  Plum BodyLovin'  Pureplay Skin Sciences India Pvt. Ltd.   \n",
       "4         Card         Flipkart                      Dummy Manufacturer   \n",
       "\n",
       "  l0_category   l1_category   l2_category l0_category_id l1_category_id  \\\n",
       "0    Specials  Bill Breaker  Bill Breaker            343           1741   \n",
       "1    Specials    Free Store    Free Store            343           1493   \n",
       "2    Specials  Bill Breaker  Bill Breaker            343           1741   \n",
       "3    Specials    Free Store    Free Store            343           1493   \n",
       "4    Specials  Bill Breaker  Bill Breaker            343           1741   \n",
       "\n",
       "  l2_category_id  \n",
       "0           1741  \n",
       "1           1493  \n",
       "2           1741  \n",
       "3           1493  \n",
       "4           1741  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products_cleaned_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321c84e",
   "metadata": {},
   "source": [
    "## Problem 07: <br> **Discount exceeds item price**\n",
    "Transactions where discount amount > (unit_price × quantity) <br>\n",
    "This is mathematically impossible in legitimate transactions.\n",
    "\n",
    "**Proposal**: Remove these transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21611a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions where discount > unit_price × quantity: 231\n",
      "Percentage: 0.0005%\n",
      "\n",
      "Sample of impossible transactions:\n",
      "        dim_customer_key  unit_selling_price  procured_quantity  \\\n",
      "137032          16908735                19.0                  3   \n",
      "292985          17024439                39.0                  1   \n",
      "478498          18330883                50.0                  1   \n",
      "552025            815083                50.0                  1   \n",
      "1407909         12776690                52.0                  1   \n",
      "1519753           644324                95.0                  1   \n",
      "1644009           919138                20.0                  4   \n",
      "1835656            16892               242.0                  1   \n",
      "1855912           334237                56.0                  1   \n",
      "2011250         12858442               153.0                  1   \n",
      "\n",
      "         total_discount_amount  pre_discount_total  \n",
      "137032                    58.0                57.0  \n",
      "292985                    40.0                39.0  \n",
      "478498                    75.0                50.0  \n",
      "552025                    51.0                50.0  \n",
      "1407909                   76.0                52.0  \n",
      "1519753                   96.0                95.0  \n",
      "1644009                   81.0                80.0  \n",
      "1835656                  243.0               242.0  \n",
      "1855912                  106.0                56.0  \n",
      "2011250                  154.0               153.0  \n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-discount total\n",
    "df_sales_cleaned['pre_discount_total'] = df_sales_cleaned['unit_selling_price'] * df_sales_cleaned['procured_quantity']\n",
    "\n",
    "# Check for impossible discounts\n",
    "df_sales_cleaned['discount_exceeds_price'] = df_sales_cleaned['total_discount_amount'] > df_sales_cleaned['pre_discount_total']\n",
    "\n",
    "impossible_discounts = df_sales_cleaned['discount_exceeds_price'].sum()\n",
    "print(f\"Transactions where discount > unit_price × quantity: {impossible_discounts:,}\")\n",
    "print(f\"Percentage: {impossible_discounts/len(df_sales_cleaned)*100:.4f}%\")\n",
    "\n",
    "if impossible_discounts > 0:\n",
    "    print(\"\\nSample of impossible transactions:\")\n",
    "    bad_trans = df_sales_cleaned[df_sales_cleaned['discount_exceeds_price']]\n",
    "    print(bad_trans[['dim_customer_key', 'unit_selling_price', 'procured_quantity', \n",
    "                     'total_discount_amount', 'pre_discount_total']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d190a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export problematic transactions for review\n",
    "# df_sales_cleaned[df_sales_cleaned['discount_exceeds_price']].to_csv(\n",
    "#     'data/anomalies/sales_impossible_discounts.csv', index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc940e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before removal: 46,448,124\n",
      "Records after removal: 46,447,893\n",
      "Removed: 231 transactions\n"
     ]
    }
   ],
   "source": [
    "# Remove impossible discount transactions\n",
    "print(f\"Records before removal: {len(df_sales_cleaned):,}\")\n",
    "df_sales_cleaned = df_sales_cleaned[~df_sales_cleaned['discount_exceeds_price']]\n",
    "print(f\"Records after removal: {len(df_sales_cleaned):,}\")\n",
    "print(f\"Removed: {impossible_discounts:,} transactions\")\n",
    "\n",
    "# Drop helper column\n",
    "df_sales_cleaned = df_sales_cleaned.drop(columns=['pre_discount_total', 'discount_exceeds_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc546d",
   "metadata": {},
   "source": [
    "## Problem 08: <br> **Negative values in price or discount**\n",
    "Check for negative values in critical financial fields.\n",
    "\n",
    "**Proposal**: Review and remove if data errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71861ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for negative values:\n",
      "  Negative unit_selling_price: 0\n",
      "  Negative total_discount_amount: 0\n",
      "  Negative total_weighted_landing_price: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for negative values\n",
    "print(\"Checking for negative values:\")\n",
    "print(f\"  Negative unit_selling_price: {(df_sales_cleaned['unit_selling_price'] < 0).sum():,}\")\n",
    "print(f\"  Negative total_discount_amount: {(df_sales_cleaned['total_discount_amount'] < 0).sum():,}\")\n",
    "print(f\"  Negative total_weighted_landing_price: {(df_sales_cleaned['total_weighted_landing_price'] < 0).sum():,}\")\n",
    "\n",
    "# If any negatives found, investigate\n",
    "if (df_sales_cleaned['unit_selling_price'] < 0).sum() > 0:\n",
    "    print(\"\\nNegative prices found:\")\n",
    "    print(df_sales_cleaned[df_sales_cleaned['unit_selling_price'] < 0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd354b4",
   "metadata": {},
   "source": [
    "## Problem 09: <br> **Duplicate transactions**\n",
    "Check for exact duplicate records that might indicate data entry errors.\n",
    "\n",
    "**Proposal**: Keep first occurrence, remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "873941c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 46,448,124\n",
      "Complete duplicate rows: 0\n",
      "Logical duplicates (same order+product+customer): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Total records: {len(df_sales_cleaned):,}\")\n",
    "\n",
    "# Check complete duplicates\n",
    "complete_dups = df_sales_cleaned.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {complete_dups:,}\")\n",
    "\n",
    "# Check logical duplicates (same order, product, customer)\n",
    "logical_dups = df_sales_cleaned.duplicated(\n",
    "    subset=['order_id', 'product_id', 'dim_customer_key'], \n",
    "    keep=False\n",
    ").sum()\n",
    "print(f\"Logical duplicates (same order+product+customer): {logical_dups:,}\")\n",
    "\n",
    "if logical_dups > 0:\n",
    "    print(\"\\nSample of duplicate transactions:\")\n",
    "    dup_sample = df_sales_cleaned[df_sales_cleaned.duplicated(\n",
    "        subset=['order_id', 'product_id', 'dim_customer_key'], \n",
    "        keep=False\n",
    "    )].sort_values(['order_id', 'product_id']).head(10)\n",
    "    print(dup_sample[['order_id', 'dim_customer_key', 'product_id', \n",
    "                      'procured_quantity', 'unit_selling_price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3d15104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complete duplicates only (keep first)\n",
    "if complete_dups > 0:\n",
    "    df_sales_cleaned = df_sales_cleaned.drop_duplicates()\n",
    "    print(f\"Removed {complete_dups:,} complete duplicate rows\")\n",
    "    print(f\"Records remaining: {len(df_sales_cleaned):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e62a4",
   "metadata": {},
   "source": [
    "## Problem 10: <br> **Date validation**\n",
    "Check for dates outside the expected range.\n",
    "\n",
    "**Proposal**: Flag or investigate unexpected dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c4d77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime first\n",
    "df_sales_cleaned['date'] = pd.to_datetime(df_sales_cleaned['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4cfc227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2022-04-01 00:00:00 to 2022-07-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Check date range\n",
    "print(f\"Date range: {df_sales_cleaned['date'].min()} to {df_sales_cleaned['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for future dates (if running in 2022)\n",
    "expected_max = pd.to_datetime('2022-07-31')\n",
    "future_dates = (df_sales_cleaned['date'] > expected_max).sum()\n",
    "if future_dates > 0:\n",
    "    print(f\"Transactions with dates after {expected_max.date()}: {future_dates:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e665b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for very old dates\n",
    "expected_min = pd.to_datetime('2022-01-01')\n",
    "old_dates = (df_sales_cleaned['date'] < expected_min).sum()\n",
    "if old_dates > 0:\n",
    "    print(f\"Transactions with dates before {expected_min.date()}: {old_dates:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143f7b4",
   "metadata": {},
   "source": [
    "## Problem 11: <br> **Extremely high quantities**\n",
    "Quantities > 100 could be legitimate bulk orders or data entry errors.\n",
    "\n",
    "**Proposal**: Flag for review, keep if legitimate business pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "645024af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions with quantity > 100: 0\n",
      "Percentage: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# Check for extreme quantities\n",
    "high_qty = df_sales_cleaned[df_sales_cleaned['procured_quantity'] > 100]\n",
    "print(f\"Transactions with quantity > 100: {len(high_qty):,}\")\n",
    "print(f\"Percentage: {len(high_qty)/len(df_sales_cleaned)*100:.4f}%\")\n",
    "\n",
    "if len(high_qty) > 0:\n",
    "    print(f\"\\nMax quantity: {df_sales_cleaned['procured_quantity'].max():,}\")\n",
    "    print(\"\\nQuantity distribution for high-qty orders:\")\n",
    "    print(high_qty['procured_quantity'].describe())\n",
    "    \n",
    "    print(\"\\nSample of high-quantity transactions:\")\n",
    "    print(high_qty[['dim_customer_key', 'product_id', 'procured_quantity', \n",
    "                    'unit_selling_price', 'total_discount_amount']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64d211",
   "metadata": {},
   "source": [
    "## Problem 12: <br> **Extreme unit prices**\n",
    "Very high or very low unit prices may indicate errors.\n",
    "\n",
    "**Proposal**: Review products with unit_price > $10,000 or < $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "79b25776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions with unit_price > $10,000: 18\n",
      "Transactions with unit_price less than free ($0): 0\n",
      "\n",
      "Max unit price: $10,999.00\n",
      "\n",
      "Most expensive products:\n",
      "         product_id  unit_selling_price  procured_quantity\n",
      "33320228     477200             10999.0                  1\n",
      "34390044     477200             10999.0                  1\n",
      "34553524     477200             10999.0                  1\n",
      "34926372     477200             10999.0                  1\n",
      "35531005     477200             10999.0                  1\n",
      "35731501     477200             10999.0                  1\n",
      "35764464     477080             10499.0                  1\n",
      "36516144     477200             10999.0                  1\n",
      "36932045     477200             10999.0                  1\n",
      "37266424     477200             10999.0                  1\n"
     ]
    }
   ],
   "source": [
    "# Check extreme prices\n",
    "very_high_price = (df_sales_cleaned['unit_selling_price'] > 10000).sum()\n",
    "very_low_price = (df_sales_cleaned['unit_selling_price'] < 0).sum() \n",
    "\n",
    "\n",
    "print(f\"Transactions with unit_price > $10,000: {very_high_price:,}\")\n",
    "print(f\"Transactions with unit_price less than free ($0): {very_low_price:,}\")\n",
    "\n",
    "if very_high_price > 0:\n",
    "    print(f\"\\nMax unit price: ${df_sales_cleaned['unit_selling_price'].max():,.2f}\")\n",
    "    expensive = df_sales_cleaned[df_sales_cleaned['unit_selling_price'] > 10000]\n",
    "    print(\"\\nMost expensive products:\")\n",
    "    print(expensive[['product_id', 'unit_selling_price', 'procured_quantity']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d500f",
   "metadata": {},
   "source": [
    "## Problem 13: <br> **Landing price > selling price (always-loss items)**\n",
    "Items where cost consistently exceeds selling price.\n",
    "\n",
    "**Proposal**: Normal for promotions, but flag if >90% of transactions are losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ebbe739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss-making transactions: 3,976,166 (8.56%)\n",
      "\n",
      "Products sold at loss >90% of the time: 1,240\n",
      "    product_id   total_profit  transaction_count  loss_count    loss_pct\n",
      "11      100476  -39802.750840              20460       19151   93.602151\n",
      "12      100497   -2024.680410                 55          55  100.000000\n",
      "13      100498  -27453.943920                696         696  100.000000\n",
      "78        1035  -24111.832090                997         937   93.981946\n",
      "79        1036   -3284.685700                135         135  100.000000\n",
      "81        1037   -2173.700150                 91          87   95.604396\n",
      "100     104543   -6617.583907               1681        1609   95.716835\n",
      "165      10632 -160346.724820               1850        1685   91.081081\n",
      "229     108107  -39868.325369               7610        7505   98.620237\n",
      "255     108920  -80703.657770               8775        7955   90.655271\n"
     ]
    }
   ],
   "source": [
    "# Calculate revenue and profit at transaction level\n",
    "df_sales_cleaned['revenue'] = (df_sales_cleaned['unit_selling_price'] * \n",
    "                                df_sales_cleaned['procured_quantity']) - df_sales_cleaned['total_discount_amount']\n",
    "df_sales_cleaned['profit'] = df_sales_cleaned['revenue'] - df_sales_cleaned['total_weighted_landing_price']\n",
    "\n",
    "# Check always-loss transactions\n",
    "always_loss = (df_sales_cleaned['profit'] < 0).sum()\n",
    "print(f\"Loss-making transactions: {always_loss:,} ({always_loss/len(df_sales_cleaned)*100:.2f}%)\")\n",
    "\n",
    "# Check products that are ALWAYS sold at a loss\n",
    "product_loss = df_sales_cleaned.groupby('product_id').agg({\n",
    "    'profit': ['sum', 'count', lambda x: (x < 0).sum()]\n",
    "}).reset_index()\n",
    "product_loss.columns = ['product_id', 'total_profit', 'transaction_count', 'loss_count']\n",
    "product_loss['loss_pct'] = (product_loss['loss_count'] / product_loss['transaction_count'] * 100)\n",
    "\n",
    "always_loss_products = product_loss[product_loss['loss_pct'] > 90]\n",
    "print(f\"\\nProducts sold at loss >90% of the time: {len(always_loss_products):,}\")\n",
    "if len(always_loss_products) > 0:\n",
    "    print(always_loss_products.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb1924",
   "metadata": {},
   "source": [
    "## Problem 14: <br> **Inconsistent pricing for same product**\n",
    "Same product_id should have relatively stable pricing.\n",
    "\n",
    "**Proposal**: Review products with extreme price variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6fe11e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with inconsistent pricing (CV > 1): 10\n",
      "\n",
      "Sample of products with varying prices:\n",
      "      product_id      mean       std  min  max  count         cv\n",
      "15674     486125  0.001070  0.032698  0.0  1.0   2804  30.561410\n",
      "16251     488060  0.001812  0.042563  0.0  1.0    552  23.494680\n",
      "15673     486124  0.002431  0.049248  0.0  1.0  21389  20.257001\n",
      "16254     488063  0.003257  0.057073  0.0  1.0    307  17.521415\n",
      "16139     487659  0.004283  0.065309  0.0  1.0   4203  15.249765\n",
      "16250     488059  0.010929  0.147844  0.0  2.0    183  13.527749\n",
      "16253     488062  0.005650  0.075165  0.0  1.0    177  13.304135\n",
      "16140     487660  0.013859  0.116969  0.0  1.0    938   8.439774\n",
      "14232     483694  0.016327  0.130287  0.0  2.0   2205   7.980079\n",
      "14095     483436  0.038462  0.196116  0.0  1.0     26   5.099020\n",
      "\n",
      "Example: product_id 483436\n",
      "               date  unit_selling_price  total_discount_amount\n",
      "9107301  2022-04-28                 0.0                    0.0\n",
      "15266289 2022-04-28                 0.0                    0.0\n",
      "14781014 2022-04-28                 0.0                    0.0\n",
      "14671481 2022-04-28                 0.0                    0.0\n",
      "14575305 2022-04-28                 0.0                    0.0\n",
      "14057507 2022-04-28                 0.0                    0.0\n",
      "13954759 2022-04-28                 0.0                    0.0\n",
      "13863918 2022-04-28                 0.0                    0.0\n",
      "15369901 2022-04-28                 0.0                    0.0\n",
      "12246384 2022-04-28                 0.0                    0.0\n"
     ]
    }
   ],
   "source": [
    "# Check price consistency per product\n",
    "price_stats = df_sales_cleaned.groupby('product_id')['unit_selling_price'].agg([\n",
    "    'mean', 'std', 'min', 'max', 'count'\n",
    "]).reset_index()\n",
    "price_stats['cv'] = price_stats['std'] / price_stats['mean']  # Coefficient of variation\n",
    "\n",
    "# Flag products with high price variance (CV > 1) and multiple transactions\n",
    "inconsistent = price_stats[(price_stats['cv'] > 1) & (price_stats['count'] > 5)]\n",
    "print(f\"Products with inconsistent pricing (CV > 1): {len(inconsistent):,}\")\n",
    "\n",
    "if len(inconsistent) > 0:\n",
    "    print(\"\\nSample of products with varying prices:\")\n",
    "    print(inconsistent.sort_values('cv', ascending=False).head(10))\n",
    "    \n",
    "    # Show example transactions for one inconsistent product\n",
    "    sample_product = inconsistent.iloc[0]['product_id']\n",
    "    print(f\"\\nExample: product_id {sample_product}\")\n",
    "    print(df_sales_cleaned[df_sales_cleaned['product_id'] == sample_product][\n",
    "        ['date', 'unit_selling_price', 'total_discount_amount']\n",
    "    ].sort_values('date').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef2fe8",
   "metadata": {},
   "source": [
    "## Problem 15: <br> **Discount percentage patterns**\n",
    "Check if discounts follow typical retail patterns (5%, 10%, 15%, 20%, etc).\n",
    "\n",
    "**Proposal**: Flag unusual discount percentages for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "019085f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discount percentage distribution:\n",
      "count    1.073321e+06\n",
      "mean     2.340311e+01\n",
      "std      2.085118e+01\n",
      "min      1.602564e-01\n",
      "25%      1.033755e+01\n",
      "50%      2.054795e+01\n",
      "75%      2.471042e+01\n",
      "max      3.967742e+02\n",
      "Name: discount_pct, dtype: float64\n",
      "\n",
      "Non-standard discount percentages: 551,935 (51.42%)\n",
      "\n",
      "Sample of unusual discounts:\n",
      "      unit_selling_price  procured_quantity  total_discount_amount  \\\n",
      "969                 30.0                  1                   20.0   \n",
      "1688                27.0                  2                   15.0   \n",
      "1835                31.0                  1                   13.0   \n",
      "1857                29.0                  1                    2.0   \n",
      "1937                38.0                  3                   76.0   \n",
      "2072                30.0                  1                    5.0   \n",
      "2111                47.0                  1                   29.0   \n",
      "2310                85.0                  1                   49.0   \n",
      "2491               234.0                  1                   51.0   \n",
      "2645               200.0                  1                   42.0   \n",
      "\n",
      "      discount_pct  \n",
      "969      66.666667  \n",
      "1688     27.777778  \n",
      "1835     41.935484  \n",
      "1857      6.896552  \n",
      "1937     66.666667  \n",
      "2072     16.666667  \n",
      "2111     61.702128  \n",
      "2310     57.647059  \n",
      "2491     21.794872  \n",
      "2645     21.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculate discount percentage\n",
    "df_sales_cleaned['discount_pct'] = (\n",
    "    df_sales_cleaned['total_discount_amount'] / \n",
    "    (df_sales_cleaned['unit_selling_price'] * df_sales_cleaned['procured_quantity']) * 100\n",
    ").fillna(0)\n",
    "\n",
    "# Check discount distribution\n",
    "print(\"Discount percentage distribution:\")\n",
    "print(df_sales_cleaned[df_sales_cleaned['discount_pct'] > 0]['discount_pct'].describe())\n",
    "\n",
    "# Check for odd discount percentages (not multiples of 5)\n",
    "discounted = df_sales_cleaned[df_sales_cleaned['discount_pct'] > 0].copy()\n",
    "discounted['is_standard'] = discounted['discount_pct'].apply(\n",
    "    lambda x: abs(round(x / 5) * 5 - x) < 1  # Within 1% of a multiple of 5\n",
    ")\n",
    "non_standard = (~discounted['is_standard']).sum()\n",
    "print(f\"\\nNon-standard discount percentages: {non_standard:,} ({non_standard/len(discounted)*100:.2f}%)\")\n",
    "\n",
    "if non_standard > 0:\n",
    "    print(\"\\nSample of unusual discounts:\")\n",
    "    print(discounted[~discounted['is_standard']][\n",
    "        ['unit_selling_price', 'procured_quantity', 'total_discount_amount', 'discount_pct']\n",
    "    ].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676fa2a",
   "metadata": {},
   "source": [
    "## Problem 16: <br> **Order-level anomalies**\n",
    "Check for suspicious patterns in order composition.\n",
    "\n",
    "**Proposal**: Flag potential data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders with multiple customers: 0\n",
      "Orders with negative total revenue: 42\n",
      "Orders with 1 item but quantity >50: 0\n"
     ]
    }
   ],
   "source": [
    "# Analyze order-level patterns\n",
    "order_stats = df_sales_cleaned.groupby('order_id').agg({\n",
    "    'product_id': 'count',\n",
    "    'procured_quantity': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'dim_customer_key': 'nunique'\n",
    "}).reset_index()\n",
    "order_stats.columns = ['order_id', 'items', 'total_qty', 'total_revenue', 'customers']\n",
    "\n",
    "# Check for orders with multiple customers (should be 1)\n",
    "multi_customer_orders = (order_stats['customers'] > 1).sum()\n",
    "print(f\"Orders with multiple customers: {multi_customer_orders:,}\")\n",
    "if multi_customer_orders > 0:\n",
    "    print(\"Potential data integrity issue\")\n",
    "\n",
    "# Check for orders with negative total revenue\n",
    "negative_revenue_orders = (order_stats['total_revenue'] < 0).sum()\n",
    "print(f\"Orders with negative total revenue: {negative_revenue_orders:,}\")\n",
    "\n",
    "# Check for orders with 1 item but quantity > 50\n",
    "suspicious_orders = ((order_stats['items'] == 1) & (order_stats['total_qty'] > 50)).sum()\n",
    "print(f\"Orders with 1 item but quantity >50: {suspicious_orders:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f086ce",
   "metadata": {},
   "source": [
    "## Problem 17: <br> **Cart and Order relationship**\n",
    "Verify cart_id and order_id mapping makes sense.\n",
    "\n",
    "**Proposal**: Document any many-to-many relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "78232983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders per cart distribution:\n",
      "order_id\n",
      "1    10394260\n",
      "2       18191\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Carts with multiple orders: 2\n",
      "\n",
      "Orders with multiple carts: 1\n"
     ]
    }
   ],
   "source": [
    "# Check cart to order mapping\n",
    "cart_orders = df_sales_cleaned.groupby('cart_id')['order_id'].nunique()\n",
    "orders_per_cart = cart_orders.value_counts().sort_index()\n",
    "\n",
    "print(\"Orders per cart distribution:\")\n",
    "print(orders_per_cart.head(10))\n",
    "\n",
    "if (orders_per_cart > 1).sum() > 0:\n",
    "    print(f\"\\nCarts with multiple orders: {(orders_per_cart > 1).sum():,}\")\n",
    "\n",
    "# Check the reverse\n",
    "order_carts = df_sales_cleaned.groupby('order_id')['cart_id'].nunique()\n",
    "carts_per_order = order_carts.value_counts().sort_index()\n",
    "\n",
    "if (carts_per_order > 1).sum() > 0:\n",
    "    print(f\"\\nOrders with multiple carts: {(carts_per_order > 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b6625",
   "metadata": {},
   "source": [
    "# Export cleaned data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4391c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv full cleaned Sales data\n",
    "\n",
    "#df_sales_cleaned.to_csv('data/cleaned/sales_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e733cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv full cleaned products data\n",
    "\n",
    "#df_products_cleaned_extended[\"product_name\"] = df_products_cleaned_extended[\"product_name\"].str.strip('\"')\n",
    "#df_products_cleaned_extended.to_csv('data/cleaned/products_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
